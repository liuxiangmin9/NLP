{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用Conv1D对IMDB电影评论数据集做文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "filepath = '../jupyter_files/labeledTrainData.tsv'\n",
    "df = pd.read_csv(filepath, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据切分为训练集和测试集\n",
    "train = df.iloc[0:20000, :]\n",
    "validation = df.iloc[20000:25000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将评论词转换为token。\n",
    "max_words = 10000          # 仅保留前10000个最常见的词\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train['review'])\n",
    "validation_sequences = tokenizer.texts_to_sequences(validation['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将评论填充或截断到同一长度\n",
    "maxlen = 300\n",
    "train_features = pad_sequences(train_sequences, maxlen=maxlen, padding='post', truncating='post')\n",
    "validation_features = pad_sequences(validation_sequences, maxlen=maxlen, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练一个1维卷积网络\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "num_epochs = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=max_words,\n",
    "                    output_dim=embedding_dim,\n",
    "                    input_length=maxlen))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 0.6463 - acc: 0.6565 - val_loss: 0.5088 - val_acc: 0.7922\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 17s 109ms/step - loss: 0.3861 - acc: 0.8613 - val_loss: 0.3840 - val_acc: 0.8664\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 18s 113ms/step - loss: 0.3764 - acc: 0.9022 - val_loss: 0.5120 - val_acc: 0.8528\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 18s 115ms/step - loss: 0.2111 - acc: 0.9418 - val_loss: 0.5697 - val_acc: 0.8610\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 17s 110ms/step - loss: 0.1255 - acc: 0.9778 - val_loss: 0.6535 - val_acc: 0.8652\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 19s 123ms/step - loss: 0.0909 - acc: 0.9918 - val_loss: 0.7495 - val_acc: 0.8654\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 18s 118ms/step - loss: 0.0723 - acc: 0.9955 - val_loss: 0.7768 - val_acc: 0.8644\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 19s 124ms/step - loss: 0.0640 - acc: 0.9960 - val_loss: 0.8108 - val_acc: 0.8622\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 20s 128ms/step - loss: 0.0603 - acc: 0.9962 - val_loss: 0.8330 - val_acc: 0.8634\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 20s 126ms/step - loss: 0.0589 - acc: 0.9962 - val_loss: 0.8468 - val_acc: 0.8630\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_features, train['sentiment'],\n",
    "                    epochs=num_epochs, batch_size=batch_size,\n",
    "                    validation_data=(validation_features, validation['sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
